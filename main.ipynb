{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to get jupyter notebook to work\n",
    "# pip install jupyter\n",
    "# pip install jupyter_http_over_ws\n",
    "# jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
    "# then use the url it gives you to open the notebook in a new tab and run the cells\n",
    "\n",
    "\n",
    "!pip install matplotlib timm==0.9.5 tqdm scipy numpy tensorboardX wget scikit-image scikit-learn xformers\n",
    "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using google colab \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/Shareddrives/ARP_Hyperspectral_Algorithms/Sample_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using local jupyter notebook\n",
    "import os\n",
    "print(os.getcwd())\n",
    "directories = [d for d in os.listdir('.') if os.path.isdir(d)]\n",
    "print(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 366/366 [00:05<00:00, 63.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#crop data\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from utils.utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from loader.dataloader import ContrastiveSegDataset, CroppedDataset\n",
    "from torchvision.transforms.functional import five_crop, ten_crop\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class RandomCropComputer(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_size(img, crop_ratio):\n",
    "        if len(img.shape) == 3:\n",
    "            return [int(img.shape[1] * crop_ratio), int(img.shape[2] * crop_ratio)]\n",
    "        elif len(img.shape) == 2:\n",
    "            return [int(img.shape[0] * crop_ratio), int(img.shape[1] * crop_ratio)]\n",
    "        else:\n",
    "            raise ValueError(\"Bad image shape {}\".format(img.shape))\n",
    "\n",
    "    def __init__(self, args, dataset_name, img_set, crop_type, crop_ratio):\n",
    "        self.pytorch_data_dir = args.data_dir\n",
    "        self.crop_ratio = crop_ratio\n",
    "\n",
    "        if crop_type == 'five':\n",
    "            crop_func = lambda x: five_crop(x, self._get_size(x, crop_ratio))\n",
    "        elif crop_type == 'double':\n",
    "            crop_ratio = 0\n",
    "            crop_func = lambda x: ten_crop(x, self._get_size(x, 0.5))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.8))\n",
    "        elif crop_type == 'super':\n",
    "            crop_ratio = 0\n",
    "            crop_func = lambda x: ten_crop(x, self._get_size(x, 0.3))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.4))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.5))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.6))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.7))\n",
    "\n",
    "        if args.dataset=='coco171':\n",
    "            self.save_dir = join(\n",
    "                args.data_dir, 'cocostuff', \"cropped\", \"coco171_{}_crop_{}\".format(crop_type, crop_ratio))\n",
    "        elif args.dataset=='coco81':\n",
    "            self.save_dir = join(\n",
    "                args.data_dir, 'cocostuff', \"cropped\", \"coco81_{}_crop_{}\".format(crop_type, crop_ratio))\n",
    "        else:\n",
    "            self.save_dir = join(\n",
    "                args.data_dir, dataset_name, \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n",
    "        self.args = args\n",
    "\n",
    "        self.img_dir = join(self.save_dir, \"img\", img_set)\n",
    "        self.label_dir = join(self.save_dir, \"label\", img_set)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        os.makedirs(self.label_dir, exist_ok=True)\n",
    "\n",
    "        # train dataset\n",
    "        # print(\"Loading dataset {}...\".format(dataset_name))\n",
    "        # print(\"Crop type: {}\".format(crop_type))\n",
    "        # print(\"Crop ratio: {}\".format(crop_ratio))\n",
    "        # print(\"data dir: {}\".format(args.data_dir))\n",
    "              \n",
    "        self.dataset = ContrastiveSegDataset(\n",
    "            pytorch_data_dir=args.data_dir,\n",
    "            dataset_name=args.dataset,\n",
    "            crop_type=crop_type,\n",
    "            image_set=img_set,\n",
    "            transform=T.ToTensor(),\n",
    "            target_transform=ToTargetTensor(),\n",
    "            extra_transform=crop_func\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return self.dataset[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def my_app():\n",
    "\n",
    "    #  note that in the dataloader.py contrastivesegdataset class, you need to hard code some struff right now if using a custom dataset\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # fixed parameter\n",
    "    parser.add_argument('--num_workers', default=int(os.cpu_count() / 8), type=int)\n",
    "\n",
    "    # dataset and baseline\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='freiburg', type=str)\n",
    "    parser.add_argument('--gpu', default=0, type=int)\n",
    "    parser.add_argument('--distributed', default='false', type=str2bool)\n",
    "    parser.add_argument('--crop_type', default='five', type=str)\n",
    "    parser.add_argument('--crop_ratio', default=0.5, type=float)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    counter = 0\n",
    "    dataset = RandomCropComputer(args, args.dataset, \"train\", args.crop_type, args.crop_ratio)\n",
    "    loader = DataLoader(dataset, 1, shuffle=False, num_workers=args.num_workers, collate_fn=lambda l: l)\n",
    "    for batch in tqdm(loader):\n",
    "        imgs = batch[0]['img']\n",
    "        # print('here')\n",
    "        labels = batch[0]['label']\n",
    "        for img, label in zip(imgs, labels):\n",
    "            img_arr = img.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "            label_arr = (label + 1).unsqueeze(0).permute(1, 2, 0).to('cpu', torch.uint8).numpy().squeeze(-1)\n",
    "            Image.fromarray(img_arr).save(join(dataset.img_dir, \"{}.jpg\".format(counter)), 'JPEG')\n",
    "            Image.fromarray(label_arr).save(join(dataset.label_dir, \"{}.png\".format(counter)), 'PNG')\n",
    "            counter+=1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "epoch: 1\n",
      "distributed: False\n",
      "load_segment: False\n",
      "load_cluster: False\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 2\n",
      "data_dir: ../\n",
      "dataset: freiburg\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "gpu: 0\n",
      "port: 12355\n",
      "grid: True\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "Already Exists!!\n"
     ]
    }
   ],
   "source": [
    "# train mediator python file \n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from modules.segment_module import compute_modularity_based_codebook\n",
    "from loader.dataloader import dataloader\n",
    "from loader.netloader import network_loader, cluster_mlp_loader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# from loader.netloader import network_loader, cluster_mlp_loader\n",
    "\n",
    "cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "def ddp_setup(args, rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = args.port\n",
    "\n",
    "    # initialize\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def ddp_clean():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "@Wrapper.EpochPrint\n",
    "def train(args, net, cluster, train_loader, optimizer):\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        img = batch[\"img\"].cuda()\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "            feat = net(img)[:, 1:, :]\n",
    "\n",
    "            # computing modularity based codebook\n",
    "            loss_mod = compute_modularity_based_codebook(cluster.codebook, feat, grid=args.grid)\n",
    "\n",
    "        # optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss_mod).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[Train]'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "def main(rank, args, ngpus_per_node):\n",
    "    # setup ddp process\n",
    "    if args.distributed: ddp_setup(args, rank, ngpus_per_node)\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, _, sampler = dataloader(args)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    cluster = cluster_mlp_loader(args, rank)\n",
    "\n",
    "    # distributed parsing\n",
    "    if args.distributed: net = net.module; cluster = cluster.module\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(cluster.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "\n",
    "    ###################################################################################\n",
    "    # train only modularity?\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if not is_exist:\n",
    "        rprint(\"No File Exists!!\", rank)\n",
    "        # train\n",
    "        for epoch in range(args.epoch):\n",
    "\n",
    "            # for shuffle\n",
    "            if args.distributed: sampler.set_epoch(epoch)\n",
    "\n",
    "            # train\n",
    "            train(\n",
    "                epoch,  # for decorator\n",
    "                rank,  # for decorator\n",
    "                args,\n",
    "                net,\n",
    "                cluster,\n",
    "                train_loader,\n",
    "                optimizer)\n",
    "\n",
    "            # scheduler step\n",
    "            scheduler.step()\n",
    "\n",
    "            # save\n",
    "            if rank == 0:\n",
    "                np.save(path, cluster.codebook.detach().cpu().numpy()\n",
    "                if args.distributed else cluster.codebook.detach().cpu().numpy())\n",
    "\n",
    "            # Interrupt for sync GPU Process\n",
    "            if args.distributed: dist.barrier()\n",
    "\n",
    "    else:\n",
    "        rprint(\"Already Exists!!\", rank)\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "    # clean ddp process\n",
    "    if args.distributed: ddp_clean()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    #  note that in the dataloader.py dataloader class, you need to hard code some struff right now if using a custom dataset\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # fixed parameter\n",
    "    parser.add_argument('--epoch', default=1, type=int)\n",
    "    # parser.add_argument('--distributed', default=True, type=str2bool)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "\n",
    "    parser.add_argument('--load_segment', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=False, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--num_workers', default=int(os.cpu_count() / 8), type=int)\n",
    "\n",
    "    # dataset and baseline\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    # parser.add_argument('--dataset', default='cocostuff27', type=str)\n",
    "    parser.add_argument('--dataset', default='freiburg', type=str)\n",
    "\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "\n",
    "    # DDP\n",
    "    # parser.add_argument('--gpu', default='0,1,2,3', type=str)\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "\n",
    "    # parameter\n",
    "    parser.add_argument('--grid', default='yes', type=str2bool)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    if args.distributed:\n",
    "        # cuda visible devices\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "        # multiprocess spawn\n",
    "        mp.spawn(main, args=(args, ngpus_per_node), nprocs=ngpus_per_node, join=True)\n",
    "    else:\n",
    "        # first gpu index is activated once there are several gpu in args.gpu\n",
    "        main(rank=gpu_list[0], args=args, ngpus_per_node=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "NAME_TAG: CAUSE-TR\n",
      "data_dir: ../\n",
      "dataset: freiburg\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "epoch: 2\n",
      "distributed: False\n",
      "load_segment: False\n",
      "load_cluster: False\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 1\n",
      "gpu: 0\n",
      "port: 12355\n",
      "grid: True\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "num_queries: 1600\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "Modularity CAUSE/freiburg/modularity/dino_vit_base_8/2048/modular.npy loaded\n",
      "-------------TRAIN EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: 7.22=6.56+0.66 ACC: 77.5%: 100%|██████████| 23/23 [00:10<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0%: 100%|██████████| 23/23 [00:05<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 0: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/segment_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Loss: 6.52=6.50+0.02 ACC: 100.0%: 100%|██████████| 23/23 [00:09<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0%: 100%|██████████| 23/23 [00:05<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 1: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/segment_tr.pth-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train front door tr python file\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from modules.segment_module import stochastic_sampling, ema_init, ema_update\n",
    "from loader.dataloader import dataloader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from loader.netloader import network_loader, segment_tr_loader, cluster_tr_loader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "# tensorboard\n",
    "counter = 0\n",
    "counter_test = 0\n",
    "\n",
    "def ddp_setup(args, rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = args.port\n",
    "\n",
    "    # initialize\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "def ddp_clean():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "@Wrapper.EpochPrint\n",
    "def train(args, net, segment, cluster, train_loader, optimizer_segment, writer, rank):\n",
    "    global counter\n",
    "    segment.train()\n",
    "\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_loss_front = 0\n",
    "    total_loss_linear = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "\n",
    "        # optimizer\n",
    "        with autocast():\n",
    "\n",
    "            # image and label and self supervised feature\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            # intermediate features\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            \n",
    "            ######################################################################\n",
    "            # teacher\n",
    "            seg_feat_ema = segment.head_ema(feat, drop=segment.dropout)\n",
    "            proj_feat_ema = segment.projection_head_ema(seg_feat_ema)\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # student\n",
    "            seg_feat = segment.head(feat, drop=segment.dropout)\n",
    "            proj_feat = segment.projection_head(seg_feat)\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # grid\n",
    "            if args.grid:\n",
    "                feat, order = stochastic_sampling(feat)\n",
    "                proj_feat, _ = stochastic_sampling(proj_feat, order=order)\n",
    "                proj_feat_ema, _ = stochastic_sampling(proj_feat_ema, order=order)\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # bank compute and contrastive loss\n",
    "            cluster.bank_compute()\n",
    "            loss_front = cluster.contrastive_ema_with_codebook_bank(feat, proj_feat, proj_feat_ema)\n",
    "            ######################################################################\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, args.n_classes)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "            loss_linear = F.cross_entropy(flat_linear_logits[flat_label_mask], flat_label[flat_label_mask])\n",
    "\n",
    "            # loss\n",
    "            loss = loss_front + loss_linear\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_segment.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        if args.dataset=='cityscapes':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 1)\n",
    "        elif args.dataset=='cocostuff27':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 2)\n",
    "        else:\n",
    "            # raise NotImplementedError\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 2) #set to two since it is the default \n",
    "        scaler.step(optimizer_segment)\n",
    "        scaler.update()\n",
    "\n",
    "        # ema update\n",
    "        ema_update(segment.head, segment.head_ema)\n",
    "        ema_update(segment.projection_head, segment.projection_head_ema)\n",
    "\n",
    "        # bank update\n",
    "        cluster.bank_update(feat, proj_feat_ema)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.view(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # loss check\n",
    "        total_loss += loss.item()\n",
    "        total_loss_front += loss_front.item()\n",
    "        total_loss_linear += loss_linear.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[Train] Loss: {total_loss / (idx + 1):.2f}={total_loss_front / (idx + 1):.2f}+{total_loss_linear / (idx + 1):.2f}'\n",
    "        desc += f' ACC: {100. * total_acc / (idx + 1):.1f}%'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "\n",
    "        # tensorboard\n",
    "        if (args.distributed == True) and (rank == 0):\n",
    "            writer.add_scalar('Train/Contrastive', loss_front, counter)\n",
    "            writer.add_scalar('Train/Linear', loss_linear, counter)\n",
    "            writer.add_scalar('Train/Acc', total_acc / (idx + 1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "@Wrapper.TestPrint\n",
    "def test(args, net, segment, nice, test_loader):\n",
    "    global counter_test\n",
    "    segment.eval()\n",
    "\n",
    "    total_acc = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_label = label.view(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.view(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[TEST] Acc (Linear): {100. * total_acc / (idx + 1):.1f}%'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaluation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "    # Interrupt for sync GPU Process\n",
    "    if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "def main(rank, args, ngpus_per_node):\n",
    "\n",
    "    # setup ddp process\n",
    "    if args.distributed: ddp_setup(args, rank, ngpus_per_node)\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, test_loader, sampler = dataloader(args)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    segment = segment_tr_loader(args, rank)\n",
    "    cluster = cluster_tr_loader(args, rank)\n",
    "\n",
    "    # distributed parsing\n",
    "    if args.distributed: net = net.module; segment = segment.module; cluster = cluster.module\n",
    "\n",
    "    # Bank and EMA\n",
    "    cluster.bank_init()\n",
    "    ema_init(segment.head, segment.head_ema)\n",
    "    ema_init(segment.projection_head, segment.projection_head_ema)\n",
    "\n",
    "    ###################################################################################\n",
    "    # First, run train_mediator.py\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if is_exist:\n",
    "        # load\n",
    "        codebook = np.load(path)\n",
    "        cluster.codebook.data = torch.from_numpy(codebook).cuda()\n",
    "        cluster.codebook.requires_grad = False\n",
    "        segment.head.codebook = torch.from_numpy(codebook).cuda()\n",
    "        segment.head_ema.codebook = torch.from_numpy(codebook).cuda()\n",
    "\n",
    "        # print successful loading modularity\n",
    "        rprint(f'Modularity {path} loaded', rank)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    else:\n",
    "        rprint('Train Modularity-based Codebook First', rank)\n",
    "        return\n",
    "    ###################################################################################\n",
    "\n",
    "    # optimizer\n",
    "    if args.dataset=='cityscapes':\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    else:\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node, weight_decay=1e-4)\n",
    "\n",
    "    # tensorboard\n",
    "    if (args.distributed == True) and (rank == 0):\n",
    "        from datetime import datetime\n",
    "        log_dir = os.path.join('logs',\n",
    "                               datetime.today().strftime(\" %m:%d_%H:%M\")[2:],\n",
    "                               args.dataset,\n",
    "                               \"_\".join(\n",
    "            [args.ckpt.split('/')[-1].split('.')[0],\n",
    "             str(args.num_codebook),\n",
    "             os.path.abspath(__file__).split('/')[-1]]))\n",
    "        check_dir(log_dir)\n",
    "    writer = SummaryWriter(log_dir=log_dir) if (rank == 0) and (args.distributed == True) else None\n",
    "\n",
    "    # evaluation\n",
    "    nice = NiceTool(args.n_classes)\n",
    "\n",
    "\n",
    "    # train\n",
    "    for epoch in range(args.epoch):\n",
    "\n",
    "        # for shuffle\n",
    "        if args.distributed: sampler.set_epoch(epoch)\n",
    "\n",
    "\n",
    "        # train\n",
    "        train(\n",
    "            epoch,  # for decorator\n",
    "            rank,  # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            cluster,\n",
    "            train_loader,\n",
    "            optimizer_segment,\n",
    "            writer, rank)\n",
    "\n",
    "\n",
    "        test(\n",
    "            epoch, # for decorator\n",
    "            rank, # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            nice,\n",
    "            test_loader)\n",
    "\n",
    "        if (rank == 0):\n",
    "            x = segment.state_dict()\n",
    "            baseline = args.ckpt.split('/')[-1].split('.')[0]\n",
    "\n",
    "            # filepath hierarchy\n",
    "            check_dir(f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}')\n",
    "\n",
    "            # save path\n",
    "            y = f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}/segment_tr.pth'\n",
    "            torch.save(x, y)\n",
    "            print(f'-----------------TEST Epoch {epoch}: SAVING CHECKPOINT IN {y}-----------------')\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    # Closing DDP\n",
    "    if args.distributed: dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model parameter\n",
    "    parser.add_argument('--NAME-TAG', default='CAUSE-TR', type=str)\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='freiburg', type=str)\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "    parser.add_argument('--epoch', default=2, type=int)\n",
    "    # parser.add_argument('--distributed', default=True, type=str2bool)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_segment', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=False, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    # parser.add_argument('--num_workers', default=int(os.cpu_count() / 8), type=int)\n",
    "    parser.add_argument('--num_workers', default=1, type=int)\n",
    "\n",
    "    # DDP\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "    \n",
    "    # codebook parameter\n",
    "    parser.add_argument('--grid', default='yes', type=str2bool)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "    args.num_queries=args.train_resolution**2 // int(args.ckpt.split('_')[-1].split('.')[0])**2\n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    if args.distributed:\n",
    "        # cuda visible devices\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "        # multiprocess spawn\n",
    "        mp.spawn(main, args=(args, ngpus_per_node), nprocs=ngpus_per_node, join=True)\n",
    "    else:\n",
    "        # first gpu index is activated once there are several gpu in args.gpu\n",
    "        main(rank=gpu_list[0], args=args, ngpus_per_node=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "NAME_TAG: CAUSE-TR\n",
      "data_dir: ../\n",
      "dataset: freiburg\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "epoch: 5\n",
      "distributed: False\n",
      "load_segment: True\n",
      "load_cluster: False\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 3\n",
      "gpu: 0\n",
      "port: 12355\n",
      "grid: True\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "num_queries: 1600\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "[Segment] CAUSE/freiburg/dino_vit_base_8/2048/segment_tr.pth loaded\n",
      "Modularity CAUSE/freiburg/modularity/dino_vit_base_8/2048/modular.npy loaded\n",
      "-------------TRAIN EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: -0.00=0.00-0.00 ACC: 100.0%: 100%|██████████| 23/23 [01:59<00:00,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0% | [mIoU]: 14.9, [mAP]: 25.0, [Acc]: 59.5, : 100%|██████████| 23/23 [02:37<00:00,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 0: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Loss: -0.00=0.00-0.00 ACC: 100.0%: 100%|██████████| 23/23 [01:50<00:00,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0% | [mIoU]: 20.5, [mAP]: 33.3, [Acc]: 61.5, : 100%|██████████| 23/23 [02:11<00:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 1: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 3-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Loss: -0.00=0.00-0.00 ACC: 100.0%: 100%|██████████| 23/23 [02:02<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 3-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0% | [mIoU]: 26.1, [mAP]: 50.0, [Acc]: 52.3, : 100%|██████████| 23/23 [02:04<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 2: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 4-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Loss: -0.00=0.00-0.00 ACC: 100.0%: 100%|██████████| 23/23 [02:05<00:00,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 4-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0% | [mIoU]: 28.2, [mAP]: 50.0, [Acc]: 56.3, : 100%|██████████| 23/23 [02:20<00:00,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 3: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 5-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Loss: -0.00=0.00-0.00 ACC: 100.0%: 100%|██████████| 23/23 [01:54<00:00,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 5-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 100.0% | [mIoU]: 28.1, [mAP]: 50.0, [Acc]: 56.3, : 100%|██████████| 23/23 [01:49<00:00,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 4: SAVING CHECKPOINT IN CAUSE/freiburg/dino_vit_base_8/2048/cluster_tr.pth-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fine tuning tr python file\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.nn.init\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "from modules.segment_module import transform, untransform, compute_modularity_based_codebook\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from loader.dataloader import dataloader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from loader.netloader import network_loader, segment_tr_loader, cluster_tr_loader\n",
    "\n",
    "cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "cmap = create_pascal_label_colormap()\n",
    "\n",
    "def ddp_setup(args, rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = args.port\n",
    "\n",
    "    # initialize\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "def ddp_clean():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "@Wrapper.EpochPrint\n",
    "def train(args, net, segment, cluster, train_loader, optimizer_segment, optimizer_cluster):\n",
    "    global counter\n",
    "    segment.train()\n",
    "\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_loss_linear = 0\n",
    "    total_loss_mod = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "\n",
    "        # optimizer\n",
    "        with autocast():\n",
    "\n",
    "            # image and label and self supervised feature\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            # intermediate features\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat, segment.dropout)\n",
    "\n",
    "            # computing modularity based codebook\n",
    "            loss_mod = compute_modularity_based_codebook(cluster.cluster_probe, seg_feat_ema, grid=args.grid)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, args.n_classes)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "            loss_linear = F.cross_entropy(flat_linear_logits[flat_label_mask], flat_label[flat_label_mask])\n",
    "\n",
    "            # loss\n",
    "            loss = loss_linear + loss_mod\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_segment.zero_grad()\n",
    "        optimizer_cluster.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        if args.dataset=='cityscapes':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 1)\n",
    "        elif args.dataset=='cocostuff27':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 2)\n",
    "        scaler.step(optimizer_segment)\n",
    "        scaler.step(optimizer_cluster)\n",
    "        scaler.update()\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.reshape(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # loss check\n",
    "        total_loss += loss.item()\n",
    "        total_loss_linear += loss_linear.item()\n",
    "        total_loss_mod += loss_mod.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[Train] Loss: {total_loss / (idx + 1):.2f}={total_loss_linear / (idx + 1):.2f}{total_loss_mod / (idx + 1):.2f}'\n",
    "        desc += f' ACC: {100. * total_acc / (idx + 1):.1f}%'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "@Wrapper.TestPrint\n",
    "def test(args, net, segment, cluster, nice, test_loader):\n",
    "    global counter_test\n",
    "    segment.eval()\n",
    "\n",
    "    total_acc = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat_ema), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # cluster\n",
    "            cluster_preds = cluster.forward_centroid(untransform(interp_seg_feat), inference=True)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.reshape(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # nice evaluation\n",
    "        _, desc_nice = nice.eval(cluster_preds, label)\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[TEST] Acc (Linear): {100. * total_acc / (idx + 1):.1f}% | {desc_nice}'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "    # Interrupt for sync GPU Process\n",
    "    if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(rank, args, ngpus_per_node):\n",
    "\n",
    "    # setup ddp process\n",
    "    if args.distributed: ddp_setup(args, rank, ngpus_per_node)\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, test_loader, sampler = dataloader(args)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    segment = segment_tr_loader(args, rank)\n",
    "    cluster = cluster_tr_loader(args, rank)\n",
    "\n",
    "    # distributed parsing\n",
    "    if args.distributed: net = net.module; segment = segment.module; cluster = cluster.module\n",
    "\n",
    "    # optimizer\n",
    "    if args.dataset=='cityscapes':\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "        optimizer_cluster = torch.optim.Adam(cluster.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    else:\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node, weight_decay=1e-4)\n",
    "        optimizer_cluster = torch.optim.Adam(cluster.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    \n",
    "    # scheduler\n",
    "    scheduler_segment = torch.optim.lr_scheduler.StepLR(optimizer_segment, step_size=2, gamma=0.5)\n",
    "    scheduler_cluster = torch.optim.lr_scheduler.StepLR(optimizer_cluster, step_size=2, gamma=0.5)\n",
    "\n",
    "    # evaluation\n",
    "    nice = NiceTool(args.n_classes)\n",
    "\n",
    "    ###################################################################################\n",
    "    # First, run train_mediator.py\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if is_exist:\n",
    "        # load\n",
    "        codebook = np.load(path)\n",
    "        cb = torch.from_numpy(codebook).cuda()\n",
    "        cluster.codebook.data = cb\n",
    "        cluster.codebook.requires_grad = False\n",
    "        segment.head.codebook = cb\n",
    "        segment.head_ema.codebook = cb\n",
    "\n",
    "        # print successful loading modularity\n",
    "        rprint(f'Modularity {path} loaded', rank)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    else:\n",
    "        rprint('Train Modularity-based Codebook First', rank)\n",
    "        return\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "    # train\n",
    "    for epoch in range(args.epoch):\n",
    "\n",
    "        # for shuffle\n",
    "        if args.distributed: sampler.set_epoch(epoch)\n",
    "\n",
    "\n",
    "        # train\n",
    "        train(\n",
    "            epoch,  # for decorator\n",
    "            rank,  # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            cluster,\n",
    "            train_loader,\n",
    "            optimizer_segment,\n",
    "            optimizer_cluster)\n",
    "\n",
    "        test(\n",
    "            epoch, # for decorator\n",
    "            rank, # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            cluster,\n",
    "            nice,\n",
    "            test_loader)\n",
    "\n",
    "        scheduler_segment.step()\n",
    "        scheduler_cluster.step()\n",
    "\n",
    "        if (rank == 0):\n",
    "            baseline = args.ckpt.split('/')[-1].split('.')[0]\n",
    "\n",
    "            # filepath hierarchy\n",
    "            check_dir(f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}')\n",
    "\n",
    "            # save path\n",
    "            y = f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}/segment_tr.pth'\n",
    "            torch.save(segment.state_dict(), y)\n",
    "\n",
    "            y = f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}/cluster_tr.pth'\n",
    "            torch.save(cluster.state_dict(), y)\n",
    "            print(f'-----------------TEST Epoch {epoch}: SAVING CHECKPOINT IN {y}-----------------')\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    # Closing DDP\n",
    "    if args.distributed: dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model parameter\n",
    "    parser.add_argument('--NAME-TAG', default='CAUSE-TR', type=str)\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='freiburg', type=str)\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "    parser.add_argument('--epoch', default=5, type=int)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_segment', default=True, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=False, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--num_workers', default=3, type=int)\n",
    "\n",
    "    # DDP\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "    \n",
    "    # codebook parameter\n",
    "    parser.add_argument('--grid', default='yes', type=str2bool)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "    args.num_queries=args.train_resolution**2 // int(args.ckpt.split('_')[-1].split('.')[0])**2\n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    if args.distributed:\n",
    "        # cuda visible devices\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "        # multiprocess spawn\n",
    "        mp.spawn(main, args=(args, ngpus_per_node), nprocs=ngpus_per_node, join=True)\n",
    "    else:\n",
    "        # first gpu index is activated once there are several gpu in args.gpu\n",
    "        main(rank=gpu_list[0], args=args, ngpus_per_node=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "NAME_TAG: CAUSE-TR\n",
      "data_dir: ../\n",
      "dataset: freiburg\n",
      "port: 12355\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "distributed: False\n",
      "load_segment: True\n",
      "load_cluster: True\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 1\n",
      "gpu: 0\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "num_queries: 1600\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "[Segment] CAUSE/freiburg/dino_vit_base_8/2048/segment_tr.pth loaded\n",
      "[Cluster] CAUSE/freiburg/dino_vit_base_8/2048/cluster_tr.pth loaded\n",
      "Modularity CAUSE/freiburg/modularity/dino_vit_base_8/2048/modular.npy loaded\n",
      "# of Parameters: 9.84(M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TEST] Acc (Linear): 100.0% | [mIoU]: 28.1, [mAP]: 50.0, [Acc]: 56.3, : 100%|██████████| 23/23 [02:22<00:00,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done test_without_crf, starting test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[mIoU]: 29.4, [mAP]: 50.0, [Acc]: 58.7, : 100%|██████████| 23/23 [08:22<00:00, 21.84s/it]\n"
     ]
    }
   ],
   "source": [
    "# test_tr file here \n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "from modules.segment_module import transform, untransform\n",
    "from loader.dataloader import dataloader\n",
    "from torch.cuda.amp import autocast\n",
    "from loader.netloader import network_loader, segment_tr_loader, cluster_tr_loader\n",
    "\n",
    "\n",
    "def test(args, net, segment, cluster, nice, test_loader, cmap):\n",
    "    segment.eval()\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    # originally Pool(40), but most computers do not have 40 cores\n",
    "    \n",
    "    # with Pool(20) as pool:\n",
    "    for _, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        ind = batch[\"ind\"].cuda()\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "        \n",
    "        # print('starting autocast')\n",
    "        with autocast():\n",
    "            # intermediate feature\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            feat_flip = net(img.flip(dims=[3]))[:, 1:, :]\n",
    "        seg_feat = transform(segment.head_ema(feat))\n",
    "        seg_feat_flip = transform(segment.head_ema(feat_flip))\n",
    "        seg_feat = untransform((seg_feat + seg_feat_flip.flip(dims=[3])) / 2)\n",
    "\n",
    "        # print('starting interp')\n",
    "        # interp feat\n",
    "        interp_seg_feat = F.interpolate(transform(seg_feat), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "# \n",
    "        # print('starting cluster')\n",
    "        # cluster preds\n",
    "        cluster_preds = cluster.forward_centroid(untransform(interp_seg_feat), crf=True)\n",
    "\n",
    "        # print('starting crf')\n",
    "        # crf\n",
    "        # crf_preds = do_crf(pool, img, cluster_preds).argmax(1).cuda()\n",
    "        crf_preds = do_crf( img, cluster_preds).argmax(1).cuda()\n",
    "\n",
    "        # print('starting nice')\n",
    "        # nice evaluation\n",
    "        _, desc_nice = nice.eval(crf_preds, label)\n",
    "\n",
    "        # print('starting hungarian')\n",
    "        # hungarian\n",
    "        hungarian_preds = nice.do_hungarian(crf_preds)\n",
    "\n",
    "        # print('starting save')\n",
    "        # save images\n",
    "        save_all(args, ind, img, label, cluster_preds.argmax(dim=1), crf_preds, hungarian_preds, cmap, is_tr=True)\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'{desc_nice}'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "\n",
    "def test_without_crf(args, net, segment, cluster, nice, test_loader):\n",
    "    segment.eval()\n",
    "\n",
    "    total_acc = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        ind = batch[\"ind\"].cuda()\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "\n",
    "        cmap = create_pascal_label_colormap()\n",
    "        a = invTrans(img)[0].permute(1,2,0)\n",
    "        b = cmap[label[0].cpu()]\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat_ema), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # cluster\n",
    "            cluster_preds = cluster.forward_centroid(untransform(interp_seg_feat), inference=True)\n",
    "\n",
    "            # nice evaluation\n",
    "            _, desc_nice = nice.eval(cluster_preds, label)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.reshape(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[TEST] Acc (Linear): {100. * total_acc / (idx + 1):.1f}% | {desc_nice}'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "def test_linear_without_crf(args, net, segment, nice, test_loader):\n",
    "    segment.eval()\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    with Pool(40) as pool:\n",
    "        for _, batch in prog_bar:\n",
    "            # image and label and self supervised feature\n",
    "            ind = batch[\"ind\"].cuda()\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            with autocast():\n",
    "                # intermediate feature\n",
    "                feat = net(img)[:, 1:, :]\n",
    "                feat_flip = net(img.flip(dims=[3]))[:, 1:, :]\n",
    "            seg_feat = transform(segment.head_ema(feat))\n",
    "            seg_feat_flip = transform(segment.head_ema(feat_flip))\n",
    "            seg_feat = untransform((seg_feat + seg_feat_flip.flip(dims=[3])) / 2)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # linear probe interp feat\n",
    "            linear_logits = segment.linear(untransform(interp_seg_feat))\n",
    "\n",
    "            # cluster preds\n",
    "            cluster_preds = linear_logits.argmax(dim=1)\n",
    "\n",
    "            # nice evaluation\n",
    "            _, desc_nice = nice.eval(cluster_preds, label)\n",
    "\n",
    "            # real-time print\n",
    "            desc = f'{desc_nice}'\n",
    "            prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "\n",
    "def test_linear(args, net, segment, nice, test_loader):\n",
    "    segment.eval()\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    with Pool(40) as pool:\n",
    "        for _, batch in prog_bar:\n",
    "            # image and label and self supervised feature\n",
    "            ind = batch[\"ind\"].cuda()\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            with autocast():\n",
    "                # intermediate feature\n",
    "                feat = net(img)[:, 1:, :]\n",
    "                feat_flip = net(img.flip(dims=[3]))[:, 1:, :]\n",
    "            seg_feat = transform(segment.head_ema(feat))\n",
    "            seg_feat_flip = transform(segment.head_ema(feat_flip))\n",
    "            seg_feat = untransform((seg_feat + seg_feat_flip.flip(dims=[3])) / 2)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # linear probe interp feat\n",
    "            linear_logits = segment.linear(untransform(interp_seg_feat))\n",
    "\n",
    "            # cluster preds\n",
    "            cluster_preds = torch.log_softmax(linear_logits, dim=1)\n",
    "\n",
    "            # crf\n",
    "            crf_preds = do_crf(pool, img, cluster_preds).argmax(1).cuda()\n",
    "\n",
    "            # nice evaluation\n",
    "            _, desc_nice = nice.eval(crf_preds, label)\n",
    "\n",
    "            # real-time print\n",
    "            desc = f'{desc_nice}'\n",
    "            prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "def main(rank, args):\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank=0)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, test_loader, _ = dataloader(args, False)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    segment = segment_tr_loader(args, rank)\n",
    "    cluster = cluster_tr_loader(args, rank)\n",
    "\n",
    "    # evaluation\n",
    "    nice = NiceTool(args.n_classes)\n",
    "\n",
    "    # color map\n",
    "    cmap = create_cityscapes_colormap() if args.dataset == 'cityscapes' else create_pascal_label_colormap()\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    # First, run train_mediator.py\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if is_exist:\n",
    "        # load\n",
    "        codebook = np.load(path)\n",
    "        cb = torch.from_numpy(codebook).cuda()\n",
    "        cluster.codebook.data = cb\n",
    "        cluster.codebook.requires_grad = False\n",
    "        segment.head.codebook = cb\n",
    "        segment.head_ema.codebook = cb\n",
    "\n",
    "        # print successful loading modularity\n",
    "        rprint(f'Modularity {path} loaded', rank)\n",
    "\n",
    "    else:\n",
    "        rprint('Train Modularity-based Codebook First', rank)\n",
    "        return\n",
    "    ###################################################################################\n",
    "\n",
    "    # param size\n",
    "    print(f'# of Parameters: {num_param(segment)/10**6:.2f}(M)') \n",
    "\n",
    "    # post-processing with crf and hungarian matching\n",
    "    test_without_crf(\n",
    "        args,\n",
    "        net,\n",
    "        segment,\n",
    "        cluster,\n",
    "        nice,\n",
    "        test_loader)\n",
    "\n",
    "    print('done test_without_crf, starting test')\n",
    "    # post-processing with crf and hungarian matching\n",
    "    test(\n",
    "        args,\n",
    "        net,\n",
    "        segment,\n",
    "        cluster,\n",
    "        nice,\n",
    "        test_loader,\n",
    "        cmap)\n",
    "    \n",
    "    # post-processing with crf and hungarian matching\n",
    "    # test_linear_without_crf(\n",
    "    #     args,\n",
    "    #     net,\n",
    "    #     segment,\n",
    "    #     nice,\n",
    "    #     test_loader)\n",
    "    \n",
    "    # test_linear(\n",
    "    #     args,\n",
    "    #     net,\n",
    "    #     segment,\n",
    "    #     nice,\n",
    "    #     test_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # model parameter\n",
    "    parser.add_argument('--NAME-TAG', default='CAUSE-TR', type=str)\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='freiburg', type=str)\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_segment', default=True, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=True, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--num_workers', default=1, type=int)\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "    args.num_queries=args.train_resolution**2 // int(args.ckpt.split('_')[-1].split('.')[0])**2\n",
    "    \n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    # first gpu index is activated once there are several gpu in args.gpu\n",
    "    main(rank=gpu_list[0], args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
