{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/anthony/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: timm==0.9.5 in /home/anthony/.local/lib/python3.10/site-packages (0.9.5)\n",
      "Requirement already satisfied: tqdm in /home/anthony/.local/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: scipy in /home/anthony/.local/lib/python3.10/site-packages (1.11.2)\n",
      "Requirement already satisfied: numpy in /home/anthony/.local/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: tensorboardX in /home/anthony/.local/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: wget in /home/anthony/.local/lib/python3.10/site-packages (3.2)\n",
      "Requirement already satisfied: scikit-image in /home/anthony/.local/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: scikit-learn in /home/anthony/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: xformers in /home/anthony/.local/lib/python3.10/site-packages (0.0.23.post1)\n",
      "Requirement already satisfied: torchvision in /home/anthony/.local/lib/python3.10/site-packages (from timm==0.9.5) (0.16.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/anthony/.local/lib/python3.10/site-packages (from timm==0.9.5) (0.20.2)\n",
      "Requirement already satisfied: torch>=1.7 in /home/anthony/.local/lib/python3.10/site-packages (from timm==0.9.5) (2.1.2)\n",
      "Requirement already satisfied: safetensors in /home/anthony/.local/lib/python3.10/site-packages (from timm==0.9.5) (0.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from timm==0.9.5) (5.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (4.42.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/anthony/.local/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: protobuf>=4.22.3 in /home/anthony/.local/lib/python3.10/site-packages (from tensorboardX) (4.23.4)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/anthony/.local/lib/python3.10/site-packages (from scikit-image) (2023.12.9)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/anthony/.local/lib/python3.10/site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/anthony/.local/lib/python3.10/site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/anthony/.local/lib/python3.10/site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/anthony/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anthony/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (2.1.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (3.12.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (11.0.2.54)\n",
      "Requirement already satisfied: fsspec in /home/anthony/.local/lib/python3.10/site-packages (from torch>=1.7->timm==0.9.5) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/anthony/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->timm==0.9.5) (12.3.101)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: requests in /home/anthony/.local/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.5) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/anthony/.local/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm==0.9.5) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anthony/.local/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.5) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anthony/.local/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.5) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anthony/.local/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anthony/.local/lib/python3.10/site-packages (from requests->huggingface-hub->timm==0.9.5) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/anthony/.local/lib/python3.10/site-packages (from sympy->torch>=1.7->timm==0.9.5) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/lucasb-eyer/pydensecrf.git\n",
      "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-92ipkw92\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-92ipkw92\n",
      "  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit dd070546eda51e21ab772ee6f14807c7f5b1548b\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "\n",
    "#to get jupyter notebook to work\n",
    "# pip install jupyter\n",
    "# pip install jupyter_http_over_ws\n",
    "# jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0\n",
    "# then use the url it gives you to open the notebook in a new tab and run the cells\n",
    "\n",
    "\n",
    "!pip install matplotlib timm==0.9.5 tqdm scipy numpy tensorboardX wget scikit-image scikit-learn xformers\n",
    "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if using google colab \n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import os\n",
    "# os.chdir('/content/drive/Shareddrives/ARP_Hyperspectral_Algorithms/Sample_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if using local jupyter notebook\n",
    "# import os\n",
    "\n",
    "# directories = [d for d in os.listdir('.') if os.path.isdir(d)]\n",
    "# print(directories)\n",
    "\n",
    "\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# # ...\n",
    "# current_path= print(os.getcwd())\n",
    "\n",
    "# # dir_path = os.path.expanduser(\"~/HyperWorkspace/colab/atlas/label/train/\")\n",
    "# dir_path = os.path.expanduser(\"../atlas/img/train/\")\n",
    "# print(len(os.listdir(dir_path)))\n",
    "\n",
    "# # Iterate over all files in the directory\n",
    "# for filename in os.listdir(dir_path):\n",
    "#     # Check if the file is a .jpg file\n",
    "#     if filename.endswith(\".jpg\"):\n",
    "#         # Open the .jpg image file\n",
    "#         img = Image.open(os.path.join(dir_path, filename))\n",
    "#         # Convert the filename to .png\n",
    "#         png_filename = os.path.splitext(filename)[0] + \".png\"\n",
    "#         # Save the image with the new .png filename\n",
    "#         img.save(os.path.join(dir_path, png_filename))\n",
    "#         print('Saved:', os.path.join(dir_path, png_filename))\n",
    "        \n",
    "        \n",
    "# # Iterate over all files in the directory\n",
    "# for file_name in os.listdir(dir_path):\n",
    "#     # If the file name contains '-'\n",
    "#     if '-' in file_name:\n",
    "#         # Remove '-' from the file name\n",
    "#         new_file_name = file_name.replace('-', '')\n",
    "        \n",
    "#         # Get the full file paths\n",
    "#         old_file_path = os.path.join(dir_path, file_name)\n",
    "#         new_file_path = os.path.join(dir_path, new_file_name)\n",
    "        \n",
    "#         # Rename the file\n",
    "#         os.rename(old_file_path, new_file_path)\n",
    "        \n",
    "#         print('Renamed:', old_file_path, 'to', new_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15112/15112 [12:33<00:00, 20.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#crop data\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from utils.utils import *\n",
    "from torch.utils.data import DataLoader\n",
    "from loader.dataloader import ContrastiveSegDataset, CroppedDataset\n",
    "from torchvision.transforms.functional import five_crop, ten_crop\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class RandomCropComputer(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_size(img, crop_ratio):\n",
    "        if len(img.shape) == 3:\n",
    "            return [int(img.shape[1] * crop_ratio), int(img.shape[2] * crop_ratio)]\n",
    "        elif len(img.shape) == 2:\n",
    "            return [int(img.shape[0] * crop_ratio), int(img.shape[1] * crop_ratio)]\n",
    "        else:\n",
    "            raise ValueError(\"Bad image shape {}\".format(img.shape))\n",
    "\n",
    "    def __init__(self, args, dataset_name, img_set, crop_type, crop_ratio):\n",
    "        self.pytorch_data_dir = args.data_dir\n",
    "        self.crop_ratio = crop_ratio\n",
    "\n",
    "        if crop_type == 'five':\n",
    "            crop_func = lambda x: five_crop(x, self._get_size(x, crop_ratio))\n",
    "        elif crop_type == 'double':\n",
    "            crop_ratio = 0\n",
    "            crop_func = lambda x: ten_crop(x, self._get_size(x, 0.5))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.8))\n",
    "        elif crop_type == 'super':\n",
    "            crop_ratio = 0\n",
    "            crop_func = lambda x: ten_crop(x, self._get_size(x, 0.3))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.4))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.5))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.6))\\\n",
    "                                + ten_crop(x, self._get_size(x, 0.7))\n",
    "\n",
    "        if args.dataset=='coco171':\n",
    "            self.save_dir = join(\n",
    "                args.data_dir, 'cocostuff', \"cropped\", \"coco171_{}_crop_{}\".format(crop_type, crop_ratio))\n",
    "        elif args.dataset=='coco81':\n",
    "            self.save_dir = join(\n",
    "                args.data_dir, 'cocostuff', \"cropped\", \"coco81_{}_crop_{}\".format(crop_type, crop_ratio))\n",
    "        else:\n",
    "            self.save_dir = join(\n",
    "                args.data_dir, dataset_name, \"cropped\", \"{}_{}_crop_{}\".format(dataset_name, crop_type, crop_ratio))\n",
    "        self.args = args\n",
    "\n",
    "        self.img_dir = join(self.save_dir, \"img\", img_set)\n",
    "        self.label_dir = join(self.save_dir, \"label\", img_set)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        os.makedirs(self.label_dir, exist_ok=True)\n",
    "\n",
    "        # train dataset\n",
    "        # print(\"Loading dataset {}...\".format(dataset_name))\n",
    "        # print(\"Crop type: {}\".format(crop_type))\n",
    "        # print(\"Crop ratio: {}\".format(crop_ratio))\n",
    "        # print(\"data dir: {}\".format(args.data_dir))\n",
    "              \n",
    "        self.dataset = ContrastiveSegDataset(\n",
    "            pytorch_data_dir=args.data_dir,\n",
    "            dataset_name=args.dataset,\n",
    "            crop_type=crop_type,\n",
    "            image_set=img_set,\n",
    "            transform=T.ToTensor(),\n",
    "            target_transform=ToTargetTensor(),\n",
    "            extra_transform=crop_func\n",
    "        )\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        return self.dataset[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def my_app():\n",
    "\n",
    "    #  note that in the dataloader.py contrastivesegdataset class, you need to hard code some struff right now if using a custom dataset\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # fixed parameter\n",
    "    parser.add_argument('--num_workers', default=1, type=int)\n",
    "\n",
    "    # dataset and baseline\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='atlas', type=str)\n",
    "    parser.add_argument('--gpu', default=0, type=int)\n",
    "    parser.add_argument('--distributed', default='false', type=str2bool)\n",
    "    parser.add_argument('--crop_type', default='five', type=str)\n",
    "    parser.add_argument('--crop_ratio', default=0.5, type=float)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    counter = 0\n",
    "    dataset = RandomCropComputer(args, args.dataset, \"train\", args.crop_type, args.crop_ratio)\n",
    "    loader = DataLoader(dataset, 1, shuffle=False, num_workers=args.num_workers, collate_fn=lambda l: l)\n",
    "    for batch in tqdm(loader):\n",
    "        imgs = batch[0]['img']\n",
    "        # print('here')\n",
    "        labels = batch[0]['label']\n",
    "        for img, label in zip(imgs, labels):\n",
    "            img_arr = img.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "            label_arr = (label + 1).unsqueeze(0).permute(1, 2, 0).to('cpu', torch.uint8).numpy().squeeze(-1)\n",
    "            Image.fromarray(img_arr).save(join(dataset.img_dir, \"{}.jpg\".format(counter)), 'JPEG')\n",
    "            Image.fromarray(label_arr).save(join(dataset.label_dir, \"{}.png\".format(counter)), 'PNG')\n",
    "            counter+=1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "epoch: 1\n",
      "distributed: False\n",
      "load_segment: False\n",
      "load_cluster: False\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 2\n",
      "data_dir: ../\n",
      "dataset: atlas\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "gpu: 0\n",
      "port: 12355\n",
      "grid: True\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "Already Exists!!\n"
     ]
    }
   ],
   "source": [
    "# train mediator python file \n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from modules.segment_module import compute_modularity_based_codebook\n",
    "from loader.dataloader import dataloader\n",
    "from loader.netloader import network_loader, cluster_mlp_loader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# from loader.netloader import network_loader, cluster_mlp_loader\n",
    "\n",
    "cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "def ddp_setup(args, rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = args.port\n",
    "\n",
    "    # initialize\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def ddp_clean():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "@Wrapper.EpochPrint\n",
    "def train(args, net, cluster, train_loader, optimizer):\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        img = batch[\"img\"].cuda()\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "            feat = net(img)[:, 1:, :]\n",
    "\n",
    "            # computing modularity based codebook\n",
    "            loss_mod = compute_modularity_based_codebook(cluster.codebook, feat, grid=args.grid)\n",
    "\n",
    "        # optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss_mod).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[Train]'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "def main(rank, args, ngpus_per_node):\n",
    "    # setup ddp process\n",
    "    if args.distributed: ddp_setup(args, rank, ngpus_per_node)\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, _, sampler = dataloader(args)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    cluster = cluster_mlp_loader(args, rank)\n",
    "\n",
    "    # distributed parsing\n",
    "    if args.distributed: net = net.module; cluster = cluster.module\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(cluster.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.2)\n",
    "\n",
    "    ###################################################################################\n",
    "    # train only modularity?\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if not is_exist:\n",
    "        rprint(\"No File Exists!!\", rank)\n",
    "        # train\n",
    "        for epoch in range(args.epoch):\n",
    "\n",
    "            # for shuffle\n",
    "            if args.distributed: sampler.set_epoch(epoch)\n",
    "\n",
    "            # train\n",
    "            train(\n",
    "                epoch,  # for decorator\n",
    "                rank,  # for decorator\n",
    "                args,\n",
    "                net,\n",
    "                cluster,\n",
    "                train_loader,\n",
    "                optimizer)\n",
    "\n",
    "            # scheduler step\n",
    "            scheduler.step()\n",
    "\n",
    "            # save\n",
    "            if rank == 0:\n",
    "                np.save(path, cluster.codebook.detach().cpu().numpy()\n",
    "                if args.distributed else cluster.codebook.detach().cpu().numpy())\n",
    "\n",
    "            # Interrupt for sync GPU Process\n",
    "            if args.distributed: dist.barrier()\n",
    "\n",
    "    else:\n",
    "        rprint(\"Already Exists!!\", rank)\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "    # clean ddp process\n",
    "    if args.distributed: ddp_clean()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    #  note that in the dataloader.py dataloader class, you need to hard code some struff right now if using a custom dataset\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # fixed parameter\n",
    "    parser.add_argument('--epoch', default=1, type=int)\n",
    "    # parser.add_argument('--distributed', default=True, type=str2bool)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "\n",
    "    parser.add_argument('--load_segment', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=False, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--num_workers', default=int(os.cpu_count() / 8), type=int)\n",
    "\n",
    "    # dataset and baseline\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    # parser.add_argument('--dataset', default='cocostuff27', type=str)\n",
    "    parser.add_argument('--dataset', default='atlas', type=str)\n",
    "\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "\n",
    "    # DDP\n",
    "    # parser.add_argument('--gpu', default='0,1,2,3', type=str)\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "\n",
    "    # parameter\n",
    "    parser.add_argument('--grid', default='yes', type=str2bool)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    if args.distributed:\n",
    "        # cuda visible devices\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "        # multiprocess spawn\n",
    "        mp.spawn(main, args=(args, ngpus_per_node), nprocs=ngpus_per_node, join=True)\n",
    "    else:\n",
    "        # first gpu index is activated once there are several gpu in args.gpu\n",
    "        main(rank=gpu_list[0], args=args, ngpus_per_node=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "NAME_TAG: CAUSE-TR\n",
      "data_dir: ../\n",
      "dataset: atlas\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "epoch: 2\n",
      "distributed: False\n",
      "load_segment: False\n",
      "load_cluster: False\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 1\n",
      "gpu: 0\n",
      "port: 12355\n",
      "grid: True\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "num_queries: 1600\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "Modularity CAUSE/atlas/modularity/dino_vit_base_8/2048/modular.npy loaded\n",
      "-------------TRAIN EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: 7.63=6.77+0.86 ACC: 74.7%: 100%|██████████| 945/945 [09:40<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 77.7%: 100%|██████████| 945/945 [07:50<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 0: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/segment_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: 7.51=6.77+0.74 ACC: 77.5%: 100%|██████████| 945/945 [1:17:09<00:00,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 78.0%: 100%|██████████| 945/945 [09:59<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 1: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/segment_tr.pth-----------------\n"
     ]
    }
   ],
   "source": [
    "# train front door tr python file\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from modules.segment_module import stochastic_sampling, ema_init, ema_update\n",
    "from loader.dataloader import dataloader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from loader.netloader import network_loader, segment_tr_loader, cluster_tr_loader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "# tensorboard\n",
    "counter = 0\n",
    "counter_test = 0\n",
    "\n",
    "def ddp_setup(args, rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = args.port\n",
    "\n",
    "    # initialize\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "def ddp_clean():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "@Wrapper.EpochPrint\n",
    "def train(args, net, segment, cluster, train_loader, optimizer_segment, writer, rank):\n",
    "    global counter\n",
    "    segment.train()\n",
    "\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_loss_front = 0\n",
    "    total_loss_linear = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "\n",
    "        # optimizer\n",
    "        with autocast():\n",
    "\n",
    "            # image and label and self supervised feature\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            # intermediate features\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            \n",
    "            ######################################################################\n",
    "            # teacher\n",
    "            seg_feat_ema = segment.head_ema(feat, drop=segment.dropout)\n",
    "            proj_feat_ema = segment.projection_head_ema(seg_feat_ema)\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # student\n",
    "            seg_feat = segment.head(feat, drop=segment.dropout)\n",
    "            proj_feat = segment.projection_head(seg_feat)\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # grid\n",
    "            if args.grid:\n",
    "                feat, order = stochastic_sampling(feat)\n",
    "                proj_feat, _ = stochastic_sampling(proj_feat, order=order)\n",
    "                proj_feat_ema, _ = stochastic_sampling(proj_feat_ema, order=order)\n",
    "            ######################################################################\n",
    "\n",
    "            ######################################################################\n",
    "            # bank compute and contrastive loss\n",
    "            cluster.bank_compute()\n",
    "            loss_front = cluster.contrastive_ema_with_codebook_bank(feat, proj_feat, proj_feat_ema)\n",
    "            ######################################################################\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, args.n_classes)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "            loss_linear = F.cross_entropy(flat_linear_logits[flat_label_mask], flat_label[flat_label_mask])\n",
    "\n",
    "            # loss\n",
    "            loss = loss_front + loss_linear\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_segment.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        if args.dataset=='cityscapes':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 1)\n",
    "        elif args.dataset=='cocostuff27':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 2)\n",
    "        else:\n",
    "            # raise NotImplementedError\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 2) #set to two since it is the default \n",
    "        scaler.step(optimizer_segment)\n",
    "        scaler.update()\n",
    "\n",
    "        # ema update\n",
    "        ema_update(segment.head, segment.head_ema)\n",
    "        ema_update(segment.projection_head, segment.projection_head_ema)\n",
    "\n",
    "        # bank update\n",
    "        cluster.bank_update(feat, proj_feat_ema)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.view(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # loss check\n",
    "        total_loss += loss.item()\n",
    "        total_loss_front += loss_front.item()\n",
    "        total_loss_linear += loss_linear.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[Train] Loss: {total_loss / (idx + 1):.2f}={total_loss_front / (idx + 1):.2f}+{total_loss_linear / (idx + 1):.2f}'\n",
    "        desc += f' ACC: {100. * total_acc / (idx + 1):.1f}%'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "\n",
    "        # tensorboard\n",
    "        if (args.distributed == True) and (rank == 0):\n",
    "            writer.add_scalar('Train/Contrastive', loss_front, counter)\n",
    "            writer.add_scalar('Train/Linear', loss_linear, counter)\n",
    "            writer.add_scalar('Train/Acc', total_acc / (idx + 1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "@Wrapper.TestPrint\n",
    "def test(args, net, segment, nice, test_loader):\n",
    "    global counter_test\n",
    "    segment.eval()\n",
    "\n",
    "    total_acc = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_label = label.view(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.view(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[TEST] Acc (Linear): {100. * total_acc / (idx + 1):.1f}%'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaluation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "    # Interrupt for sync GPU Process\n",
    "    if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "def main(rank, args, ngpus_per_node):\n",
    "\n",
    "    # setup ddp process\n",
    "    if args.distributed: ddp_setup(args, rank, ngpus_per_node)\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, test_loader, sampler = dataloader(args)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    segment = segment_tr_loader(args, rank)\n",
    "    cluster = cluster_tr_loader(args, rank)\n",
    "\n",
    "    # distributed parsing\n",
    "    if args.distributed: net = net.module; segment = segment.module; cluster = cluster.module\n",
    "\n",
    "    # Bank and EMA\n",
    "    cluster.bank_init()\n",
    "    ema_init(segment.head, segment.head_ema)\n",
    "    ema_init(segment.projection_head, segment.projection_head_ema)\n",
    "\n",
    "    ###################################################################################\n",
    "    # First, run train_mediator.py\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if is_exist:\n",
    "        # load\n",
    "        codebook = np.load(path)\n",
    "        cluster.codebook.data = torch.from_numpy(codebook).cuda()\n",
    "        cluster.codebook.requires_grad = False\n",
    "        segment.head.codebook = torch.from_numpy(codebook).cuda()\n",
    "        segment.head_ema.codebook = torch.from_numpy(codebook).cuda()\n",
    "\n",
    "        # print successful loading modularity\n",
    "        rprint(f'Modularity {path} loaded', rank)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    else:\n",
    "        rprint('Train Modularity-based Codebook First', rank)\n",
    "        return\n",
    "    ###################################################################################\n",
    "\n",
    "    # optimizer\n",
    "    if args.dataset=='cityscapes':\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    else:\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node, weight_decay=1e-4)\n",
    "\n",
    "    # tensorboard\n",
    "    if (args.distributed == True) and (rank == 0):\n",
    "        from datetime import datetime\n",
    "        log_dir = os.path.join('logs',\n",
    "                               datetime.today().strftime(\" %m:%d_%H:%M\")[2:],\n",
    "                               args.dataset,\n",
    "                               \"_\".join(\n",
    "            [args.ckpt.split('/')[-1].split('.')[0],\n",
    "             str(args.num_codebook),\n",
    "             os.path.abspath(__file__).split('/')[-1]]))\n",
    "        check_dir(log_dir)\n",
    "    writer = SummaryWriter(log_dir=log_dir) if (rank == 0) and (args.distributed == True) else None\n",
    "\n",
    "    # evaluation\n",
    "    nice = NiceTool(args.n_classes)\n",
    "\n",
    "\n",
    "    # train\n",
    "    for epoch in range(args.epoch):\n",
    "\n",
    "        # for shuffle\n",
    "        if args.distributed: sampler.set_epoch(epoch)\n",
    "\n",
    "\n",
    "        # train\n",
    "        train(\n",
    "            epoch,  # for decorator\n",
    "            rank,  # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            cluster,\n",
    "            train_loader,\n",
    "            optimizer_segment,\n",
    "            writer, rank)\n",
    "\n",
    "\n",
    "        test(\n",
    "            epoch, # for decorator\n",
    "            rank, # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            nice,\n",
    "            test_loader)\n",
    "\n",
    "        if (rank == 0):\n",
    "            x = segment.state_dict()\n",
    "            baseline = args.ckpt.split('/')[-1].split('.')[0]\n",
    "\n",
    "            # filepath hierarchy\n",
    "            check_dir(f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}')\n",
    "\n",
    "            # save path\n",
    "            y = f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}/segment_tr.pth'\n",
    "            torch.save(x, y)\n",
    "            print(f'-----------------TEST Epoch {epoch}: SAVING CHECKPOINT IN {y}-----------------')\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    # Closing DDP\n",
    "    if args.distributed: dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model parameter\n",
    "    parser.add_argument('--NAME-TAG', default='CAUSE-TR', type=str)\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='atlas', type=str)\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "    parser.add_argument('--epoch', default=2, type=int)\n",
    "    # parser.add_argument('--distributed', default=True, type=str2bool)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_segment', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=False, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    # parser.add_argument('--num_workers', default=int(os.cpu_count() / 8), type=int)\n",
    "    parser.add_argument('--num_workers', default=1, type=int)\n",
    "\n",
    "    # DDP\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "    \n",
    "    # codebook parameter\n",
    "    parser.add_argument('--grid', default='yes', type=str2bool)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "    args.num_queries=args.train_resolution**2 // int(args.ckpt.split('_')[-1].split('.')[0])**2\n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    if args.distributed:\n",
    "        # cuda visible devices\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "        # multiprocess spawn\n",
    "        mp.spawn(main, args=(args, ngpus_per_node), nprocs=ngpus_per_node, join=True)\n",
    "    else:\n",
    "        # first gpu index is activated once there are several gpu in args.gpu\n",
    "        main(rank=gpu_list[0], args=args, ngpus_per_node=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "NAME_TAG: CAUSE-TR\n",
      "data_dir: ../\n",
      "dataset: atlas\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "epoch: 5\n",
      "distributed: False\n",
      "load_segment: True\n",
      "load_cluster: False\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 3\n",
      "gpu: 0\n",
      "port: 12355\n",
      "grid: True\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "num_queries: 1600\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "[Segment] CAUSE/atlas/dino_vit_base_8/2048/segment_tr.pth loaded\n",
      "Modularity CAUSE/atlas/modularity/dino_vit_base_8/2048/modular.npy loaded\n",
      "-------------TRAIN EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: 0.00=0.40-0.40 ACC: 87.9%: 100%|██████████| 945/945 [05:47<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 1-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 89.3% | [mIoU]: 14.1, [mAP]: 46.6, [Acc]: 69.1, : 100%|██████████| 945/945 [04:06<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 0: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Loss: -0.04=0.34-0.37 ACC: 89.5%: 100%|██████████| 945/945 [05:42<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 2-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 90.0% | [mIoU]: 17.7, [mAP]: 58.3, [Acc]: 75.6, : 100%|██████████| 945/945 [04:01<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 1: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 3-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: -0.05=0.30-0.36 ACC: 90.3%: 100%|██████████| 945/945 [05:31<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 3-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 90.4% | [mIoU]: 17.6, [mAP]: 68.0, [Acc]: 75.2, : 100%|██████████| 945/945 [04:02<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 2: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 4-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: -0.05=0.30-0.35 ACC: 90.3%: 100%|██████████| 945/945 [05:39<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 4-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 90.6% | [mIoU]: 18.0, [mAP]: 66.1, [Acc]: 76.3, : 100%|██████████| 945/945 [04:02<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 3: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/cluster_tr.pth-----------------\n",
      "-------------TRAIN EPOCH: 5-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train] Loss: -0.06=0.28-0.34 ACC: 90.7%: 100%|██████████| 945/945 [05:39<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TEST EPOCH: 5-------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[TEST] Acc (Linear): 91.0% | [mIoU]: 17.6, [mAP]: 61.1, [Acc]: 75.3, : 100%|██████████| 945/945 [04:02<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------TEST Epoch 4: SAVING CHECKPOINT IN CAUSE/atlas/dino_vit_base_8/2048/cluster_tr.pth-----------------\n"
     ]
    }
   ],
   "source": [
    "# fine tuning tr python file\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch.nn.init\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "from modules.segment_module import transform, untransform, compute_modularity_based_codebook\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from loader.dataloader import dataloader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from loader.netloader import network_loader, segment_tr_loader, cluster_tr_loader\n",
    "\n",
    "cudnn.benchmark = True\n",
    "scaler = GradScaler()\n",
    "\n",
    "cmap = create_pascal_label_colormap()\n",
    "\n",
    "def ddp_setup(args, rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = args.port\n",
    "\n",
    "    # initialize\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "\n",
    "def ddp_clean():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "@Wrapper.EpochPrint\n",
    "def train(args, net, segment, cluster, train_loader, optimizer_segment, optimizer_cluster):\n",
    "    global counter\n",
    "    segment.train()\n",
    "\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    total_loss_linear = 0\n",
    "    total_loss_mod = 0\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "\n",
    "        # optimizer\n",
    "        with autocast():\n",
    "\n",
    "            # image and label and self supervised feature\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            # intermediate features\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat, segment.dropout)\n",
    "\n",
    "            # computing modularity based codebook\n",
    "            loss_mod = compute_modularity_based_codebook(cluster.cluster_probe, seg_feat_ema, grid=args.grid)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_linear_logits = linear_logits.permute(0, 2, 3, 1).reshape(-1, args.n_classes)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "            loss_linear = F.cross_entropy(flat_linear_logits[flat_label_mask], flat_label[flat_label_mask])\n",
    "\n",
    "            # loss\n",
    "            loss = loss_linear + loss_mod\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_segment.zero_grad()\n",
    "        optimizer_cluster.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        if args.dataset=='cityscapes':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 1)\n",
    "        elif args.dataset=='cocostuff27':\n",
    "            scaler.unscale_(optimizer_segment)\n",
    "            torch.nn.utils.clip_grad_norm_(segment.parameters(), 2)\n",
    "        scaler.step(optimizer_segment)\n",
    "        scaler.step(optimizer_cluster)\n",
    "        scaler.update()\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.reshape(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # loss check\n",
    "        total_loss += loss.item()\n",
    "        total_loss_linear += loss_linear.item()\n",
    "        total_loss_mod += loss_mod.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[Train] Loss: {total_loss / (idx + 1):.2f}={total_loss_linear / (idx + 1):.2f}{total_loss_mod / (idx + 1):.2f}'\n",
    "        desc += f' ACC: {100. * total_acc / (idx + 1):.1f}%'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "@Wrapper.TestPrint\n",
    "def test(args, net, segment, cluster, nice, test_loader):\n",
    "    global counter_test\n",
    "    segment.eval()\n",
    "\n",
    "    total_acc = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat_ema), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # cluster\n",
    "            cluster_preds = cluster.forward_centroid(untransform(interp_seg_feat), inference=True)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.reshape(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # nice evaluation\n",
    "        _, desc_nice = nice.eval(cluster_preds, label)\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[TEST] Acc (Linear): {100. * total_acc / (idx + 1):.1f}% | {desc_nice}'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "    # Interrupt for sync GPU Process\n",
    "    if args.distributed: dist.barrier()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(rank, args, ngpus_per_node):\n",
    "\n",
    "    # setup ddp process\n",
    "    if args.distributed: ddp_setup(args, rank, ngpus_per_node)\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, test_loader, sampler = dataloader(args)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    segment = segment_tr_loader(args, rank)\n",
    "    cluster = cluster_tr_loader(args, rank)\n",
    "\n",
    "    # distributed parsing\n",
    "    if args.distributed: net = net.module; segment = segment.module; cluster = cluster.module\n",
    "\n",
    "    # optimizer\n",
    "    if args.dataset=='cityscapes':\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "        optimizer_cluster = torch.optim.Adam(cluster.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    else:\n",
    "        optimizer_segment = torch.optim.Adam(segment.parameters(), lr=1e-3 * ngpus_per_node, weight_decay=1e-4)\n",
    "        optimizer_cluster = torch.optim.Adam(cluster.parameters(), lr=1e-3 * ngpus_per_node)\n",
    "    \n",
    "    # scheduler\n",
    "    scheduler_segment = torch.optim.lr_scheduler.StepLR(optimizer_segment, step_size=2, gamma=0.5)\n",
    "    scheduler_cluster = torch.optim.lr_scheduler.StepLR(optimizer_cluster, step_size=2, gamma=0.5)\n",
    "\n",
    "    # evaluation\n",
    "    nice = NiceTool(args.n_classes)\n",
    "\n",
    "    ###################################################################################\n",
    "    # First, run train_mediator.py\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if is_exist:\n",
    "        # load\n",
    "        codebook = np.load(path)\n",
    "        cb = torch.from_numpy(codebook).cuda()\n",
    "        cluster.codebook.data = cb\n",
    "        cluster.codebook.requires_grad = False\n",
    "        segment.head.codebook = cb\n",
    "        segment.head_ema.codebook = cb\n",
    "\n",
    "        # print successful loading modularity\n",
    "        rprint(f'Modularity {path} loaded', rank)\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    else:\n",
    "        rprint('Train Modularity-based Codebook First', rank)\n",
    "        return\n",
    "    ###################################################################################\n",
    "\n",
    "\n",
    "    # train\n",
    "    for epoch in range(args.epoch):\n",
    "\n",
    "        # for shuffle\n",
    "        if args.distributed: sampler.set_epoch(epoch)\n",
    "\n",
    "\n",
    "        # train\n",
    "        train(\n",
    "            epoch,  # for decorator\n",
    "            rank,  # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            cluster,\n",
    "            train_loader,\n",
    "            optimizer_segment,\n",
    "            optimizer_cluster)\n",
    "\n",
    "        test(\n",
    "            epoch, # for decorator\n",
    "            rank, # for decorator\n",
    "            args,\n",
    "            net,\n",
    "            segment,\n",
    "            cluster,\n",
    "            nice,\n",
    "            test_loader)\n",
    "\n",
    "        scheduler_segment.step()\n",
    "        scheduler_cluster.step()\n",
    "\n",
    "        if (rank == 0):\n",
    "            baseline = args.ckpt.split('/')[-1].split('.')[0]\n",
    "\n",
    "            # filepath hierarchy\n",
    "            check_dir(f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}')\n",
    "\n",
    "            # save path\n",
    "            y = f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}/segment_tr.pth'\n",
    "            torch.save(segment.state_dict(), y)\n",
    "\n",
    "            y = f'CAUSE/{args.dataset}/{baseline}/{args.num_codebook}/cluster_tr.pth'\n",
    "            torch.save(cluster.state_dict(), y)\n",
    "            print(f'-----------------TEST Epoch {epoch}: SAVING CHECKPOINT IN {y}-----------------')\n",
    "\n",
    "        # Interrupt for sync GPU Process\n",
    "        if args.distributed: dist.barrier()\n",
    "\n",
    "    # Closing DDP\n",
    "    if args.distributed: dist.barrier(); dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model parameter\n",
    "    parser.add_argument('--NAME-TAG', default='CAUSE-TR', type=str)\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='atlas', type=str)\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "    parser.add_argument('--epoch', default=5, type=int)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_segment', default=True, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=False, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--num_workers', default=3, type=int)\n",
    "\n",
    "    # DDP\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "    \n",
    "    # codebook parameter\n",
    "    parser.add_argument('--grid', default='yes', type=str2bool)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "    args.num_queries=args.train_resolution**2 // int(args.ckpt.split('_')[-1].split('.')[0])**2\n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    if args.distributed:\n",
    "        # cuda visible devices\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "        # multiprocess spawn\n",
    "        mp.spawn(main, args=(args, ngpus_per_node), nprocs=ngpus_per_node, join=True)\n",
    "    else:\n",
    "        # first gpu index is activated once there are several gpu in args.gpu\n",
    "        main(rank=gpu_list[0], args=args, ngpus_per_node=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Configurations------------------\n",
      "NAME_TAG: CAUSE-TR\n",
      "data_dir: ../\n",
      "dataset: atlas\n",
      "port: 12355\n",
      "ckpt: checkpoint/dino_vit_base_8.pth\n",
      "distributed: False\n",
      "load_segment: True\n",
      "load_cluster: True\n",
      "train_resolution: 320\n",
      "test_resolution: 320\n",
      "batch_size: 16\n",
      "num_workers: 1\n",
      "gpu: 0\n",
      "num_codebook: 2048\n",
      "reduced_dim: 90\n",
      "projection_dim: 2048\n",
      "dim: 768\n",
      "num_queries: 1600\n",
      "-------------------------------------------------\n",
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "[Segment] CAUSE/atlas/dino_vit_base_8/2048/segment_tr.pth loaded\n",
      "[Cluster] CAUSE/atlas/dino_vit_base_8/2048/cluster_tr.pth loaded\n",
      "Modularity CAUSE/atlas/modularity/dino_vit_base_8/2048/modular.npy loaded\n",
      "# of Parameters: 9.84(M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TEST] Acc (Linear): 91.0% | [mIoU]: 17.6, [mAP]: 61.1, [Acc]: 75.3, : 100%|██████████| 945/945 [07:49<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done test_without_crf, starting test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[mIoU]: 17.5, [mAP]: 62.5, [Acc]: 75.8, : 100%|██████████| 945/945 [3:55:22<00:00, 14.94s/it]  \n"
     ]
    }
   ],
   "source": [
    "# test_tr file here \n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *\n",
    "from modules.segment_module import transform, untransform\n",
    "from loader.dataloader import dataloader\n",
    "from torch.cuda.amp import autocast\n",
    "from loader.netloader import network_loader, segment_tr_loader, cluster_tr_loader\n",
    "\n",
    "\n",
    "def test(args, net, segment, cluster, nice, test_loader, cmap):\n",
    "    segment.eval()\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    # originally Pool(40), but most computers do not have 40 cores\n",
    "    \n",
    "    # with Pool(20) as pool:\n",
    "    for _, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        ind = batch[\"ind\"].cuda()\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "        \n",
    "        # print('starting autocast')\n",
    "        with autocast():\n",
    "            # intermediate feature\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            feat_flip = net(img.flip(dims=[3]))[:, 1:, :]\n",
    "        seg_feat = transform(segment.head_ema(feat))\n",
    "        seg_feat_flip = transform(segment.head_ema(feat_flip))\n",
    "        seg_feat = untransform((seg_feat + seg_feat_flip.flip(dims=[3])) / 2)\n",
    "\n",
    "        # print('starting interp')\n",
    "        # interp feat\n",
    "        interp_seg_feat = F.interpolate(transform(seg_feat), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "# \n",
    "        # print('starting cluster')\n",
    "        # cluster preds\n",
    "        cluster_preds = cluster.forward_centroid(untransform(interp_seg_feat), crf=True)\n",
    "\n",
    "        # print('starting crf')\n",
    "        # crf\n",
    "        # crf_preds = do_crf(pool, img, cluster_preds).argmax(1).cuda()\n",
    "        crf_preds = do_crf( img, cluster_preds).argmax(1).cuda()\n",
    "\n",
    "        # print('starting nice')\n",
    "        # nice evaluation\n",
    "        _, desc_nice = nice.eval(crf_preds, label)\n",
    "\n",
    "        # print('starting hungarian')\n",
    "        # hungarian\n",
    "        hungarian_preds = nice.do_hungarian(crf_preds)\n",
    "\n",
    "        # print('starting save')\n",
    "        # save images\n",
    "        save_all(args, ind, img, label, cluster_preds.argmax(dim=1), crf_preds, hungarian_preds, cmap, is_tr=True)\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'{desc_nice}'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "\n",
    "def test_without_crf(args, net, segment, cluster, nice, test_loader):\n",
    "    segment.eval()\n",
    "\n",
    "    total_acc = 0\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    for idx, batch in prog_bar:\n",
    "        # image and label and self supervised feature\n",
    "        ind = batch[\"ind\"].cuda()\n",
    "        img = batch[\"img\"].cuda()\n",
    "        label = batch[\"label\"].cuda()\n",
    "\n",
    "        cmap = create_pascal_label_colormap()\n",
    "        a = invTrans(img)[0].permute(1,2,0)\n",
    "        b = cmap[label[0].cpu()]\n",
    "\n",
    "        # intermediate feature\n",
    "        with autocast():\n",
    "\n",
    "            feat = net(img)[:, 1:, :]\n",
    "            seg_feat_ema = segment.head_ema(feat)\n",
    "\n",
    "            # linear probe loss\n",
    "            linear_logits = segment.linear(seg_feat_ema)\n",
    "            linear_logits = F.interpolate(linear_logits, label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            flat_label = label.reshape(-1)\n",
    "            flat_label_mask = (flat_label >= 0) & (flat_label < args.n_classes)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat_ema), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # cluster\n",
    "            cluster_preds = cluster.forward_centroid(untransform(interp_seg_feat), inference=True)\n",
    "\n",
    "            # nice evaluation\n",
    "            _, desc_nice = nice.eval(cluster_preds, label)\n",
    "\n",
    "        # linear probe acc check\n",
    "        pred_label = linear_logits.argmax(dim=1)\n",
    "        flat_pred_label = pred_label.reshape(-1)\n",
    "        acc = (flat_pred_label[flat_label_mask] == flat_label[flat_label_mask]).sum() / flat_label[\n",
    "            flat_label_mask].numel()\n",
    "        total_acc += acc.item()\n",
    "\n",
    "        # real-time print\n",
    "        desc = f'[TEST] Acc (Linear): {100. * total_acc / (idx + 1):.1f}% | {desc_nice}'\n",
    "        prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "def test_linear_without_crf(args, net, segment, nice, test_loader):\n",
    "    segment.eval()\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    with Pool(40) as pool:\n",
    "        for _, batch in prog_bar:\n",
    "            # image and label and self supervised feature\n",
    "            ind = batch[\"ind\"].cuda()\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            with autocast():\n",
    "                # intermediate feature\n",
    "                feat = net(img)[:, 1:, :]\n",
    "                feat_flip = net(img.flip(dims=[3]))[:, 1:, :]\n",
    "            seg_feat = transform(segment.head_ema(feat))\n",
    "            seg_feat_flip = transform(segment.head_ema(feat_flip))\n",
    "            seg_feat = untransform((seg_feat + seg_feat_flip.flip(dims=[3])) / 2)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # linear probe interp feat\n",
    "            linear_logits = segment.linear(untransform(interp_seg_feat))\n",
    "\n",
    "            # cluster preds\n",
    "            cluster_preds = linear_logits.argmax(dim=1)\n",
    "\n",
    "            # nice evaluation\n",
    "            _, desc_nice = nice.eval(cluster_preds, label)\n",
    "\n",
    "            # real-time print\n",
    "            desc = f'{desc_nice}'\n",
    "            prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "\n",
    "def test_linear(args, net, segment, nice, test_loader):\n",
    "    segment.eval()\n",
    "\n",
    "    prog_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=True)\n",
    "    with Pool(40) as pool:\n",
    "        for _, batch in prog_bar:\n",
    "            # image and label and self supervised feature\n",
    "            ind = batch[\"ind\"].cuda()\n",
    "            img = batch[\"img\"].cuda()\n",
    "            label = batch[\"label\"].cuda()\n",
    "\n",
    "            with autocast():\n",
    "                # intermediate feature\n",
    "                feat = net(img)[:, 1:, :]\n",
    "                feat_flip = net(img.flip(dims=[3]))[:, 1:, :]\n",
    "            seg_feat = transform(segment.head_ema(feat))\n",
    "            seg_feat_flip = transform(segment.head_ema(feat_flip))\n",
    "            seg_feat = untransform((seg_feat + seg_feat_flip.flip(dims=[3])) / 2)\n",
    "\n",
    "            # interp feat\n",
    "            interp_seg_feat = F.interpolate(transform(seg_feat), label.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # linear probe interp feat\n",
    "            linear_logits = segment.linear(untransform(interp_seg_feat))\n",
    "\n",
    "            # cluster preds\n",
    "            cluster_preds = torch.log_softmax(linear_logits, dim=1)\n",
    "\n",
    "            # crf\n",
    "            crf_preds = do_crf(pool, img, cluster_preds).argmax(1).cuda()\n",
    "\n",
    "            # nice evaluation\n",
    "            _, desc_nice = nice.eval(crf_preds, label)\n",
    "\n",
    "            # real-time print\n",
    "            desc = f'{desc_nice}'\n",
    "            prog_bar.set_description(desc, refresh=True)\n",
    "\n",
    "    # evaludation metric reset\n",
    "    nice.reset()\n",
    "\n",
    "\n",
    "def main(rank, args):\n",
    "\n",
    "    # setting gpu id of this process\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # print argparse\n",
    "    print_argparse(args, rank=0)\n",
    "\n",
    "    # dataset loader\n",
    "    train_loader, test_loader, _ = dataloader(args, False)\n",
    "\n",
    "    # network loader\n",
    "    net = network_loader(args, rank)\n",
    "    segment = segment_tr_loader(args, rank)\n",
    "    cluster = cluster_tr_loader(args, rank)\n",
    "\n",
    "    # evaluation\n",
    "    nice = NiceTool(args.n_classes)\n",
    "\n",
    "    # color map\n",
    "    cmap = create_cityscapes_colormap() if args.dataset == 'cityscapes' else create_pascal_label_colormap()\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    # First, run train_mediator.py\n",
    "    path, is_exist = pickle_path_and_exist(args)\n",
    "\n",
    "    # early save for time\n",
    "    if is_exist:\n",
    "        # load\n",
    "        codebook = np.load(path)\n",
    "        cb = torch.from_numpy(codebook).cuda()\n",
    "        cluster.codebook.data = cb\n",
    "        cluster.codebook.requires_grad = False\n",
    "        segment.head.codebook = cb\n",
    "        segment.head_ema.codebook = cb\n",
    "\n",
    "        # print successful loading modularity\n",
    "        rprint(f'Modularity {path} loaded', rank)\n",
    "\n",
    "    else:\n",
    "        rprint('Train Modularity-based Codebook First', rank)\n",
    "        return\n",
    "    ###################################################################################\n",
    "\n",
    "    # param size\n",
    "    print(f'# of Parameters: {num_param(segment)/10**6:.2f}(M)') \n",
    "\n",
    "    # post-processing with crf and hungarian matching\n",
    "    test_without_crf(\n",
    "        args,\n",
    "        net,\n",
    "        segment,\n",
    "        cluster,\n",
    "        nice,\n",
    "        test_loader)\n",
    "\n",
    "    print('done test_without_crf, starting test')\n",
    "    # post-processing with crf and hungarian matching\n",
    "    test(\n",
    "        args,\n",
    "        net,\n",
    "        segment,\n",
    "        cluster,\n",
    "        nice,\n",
    "        test_loader,\n",
    "        cmap)\n",
    "    \n",
    "    # post-processing with crf and hungarian matching\n",
    "    # test_linear_without_crf(\n",
    "    #     args,\n",
    "    #     net,\n",
    "    #     segment,\n",
    "    #     nice,\n",
    "    #     test_loader)\n",
    "    \n",
    "    # test_linear(\n",
    "    #     args,\n",
    "    #     net,\n",
    "    #     segment,\n",
    "    #     nice,\n",
    "    #     test_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # fetch args\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # model parameter\n",
    "    parser.add_argument('--NAME-TAG', default='CAUSE-TR', type=str)\n",
    "    parser.add_argument('--data_dir', default='../', type=str)\n",
    "    parser.add_argument('--dataset', default='atlas', type=str)\n",
    "    parser.add_argument('--port', default='12355', type=str)\n",
    "    parser.add_argument('--ckpt', default='checkpoint/dino_vit_base_8.pth', type=str)\n",
    "    parser.add_argument('--distributed', default=False, type=str2bool)\n",
    "    parser.add_argument('--load_segment', default=True, type=str2bool)\n",
    "    parser.add_argument('--load_cluster', default=True, type=str2bool)\n",
    "    parser.add_argument('--train_resolution', default=320, type=int)\n",
    "    parser.add_argument('--test_resolution', default=320, type=int)\n",
    "    parser.add_argument('--batch_size', default=16, type=int)\n",
    "    parser.add_argument('--num_workers', default=1, type=int)\n",
    "    parser.add_argument('--gpu', default='0', type=str)\n",
    "    parser.add_argument('--num_codebook', default=2048, type=int)\n",
    "\n",
    "    # model parameter\n",
    "    parser.add_argument('--reduced_dim', default=90, type=int)\n",
    "    parser.add_argument('--projection_dim', default=2048, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "    if 'dinov2' in args.ckpt:\n",
    "        args.train_resolution=322\n",
    "        args.test_resolution=322\n",
    "    if 'small' in args.ckpt:\n",
    "        args.dim=384\n",
    "    elif 'base' in args.ckpt:\n",
    "        args.dim=768\n",
    "    args.num_queries=args.train_resolution**2 // int(args.ckpt.split('_')[-1].split('.')[0])**2\n",
    "    \n",
    "\n",
    "    # the number of gpus for multi-process\n",
    "    gpu_list = list(map(int, args.gpu.split(',')))\n",
    "    ngpus_per_node = len(gpu_list)\n",
    "\n",
    "    # first gpu index is activated once there are several gpu in args.gpu\n",
    "    main(rank=gpu_list[0], args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
